\title{Importance of Big data in predicting stock returns and price}


\author{Gagan Arora}
\orcid{}
\affiliation{%
  \institution{Indiana University}
  \streetaddress{2709 E 10th St}
  \city{Bloomington} 
  \state{Indiana} 
  \postcode{47401}
}
\email{gkarora@iu.edu}




% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{Gagan Arora}


\begin{abstract}
In this project, we will discuss the importance of big data in finance industry in predicting financial stock values.  
We will be using python libraries to fetch financial data from yahoo finance and will further predict the stock price returns of few selected technology
companies such as Amazon, Yahoo depending on the historical data of x[16] years. Similarly, we will predict the returns based on y[10] years of data. 
The prediction will be based on SP 500 market return and market risk volatility. 
Here y is greater than x and then we will compare the predicted returns with the current returns. For the comparison we will be using the testing time frame as mentioned
in the project later. This project will help us understand if more historic data helps in predicting the stock price returns or it adds noise. 
We will be using statistical approach and CAPM [capital asset pricing model ] to predict stock price.  Analysis will be done on the jupyter notebook
\end{abstract}

\keywords{HID-301, Stock Price prediction, stock returns, SP500, risk free market, CAPM model, root mean square analysis, stock beta, Finance, Statistics,mean, variance, market premium, python,yahoo finance, i523}


\maketitle

\section{Introduction}
By its nature of the business, the finance industry is always driven and dominated by data. 
The existence of Big data in the finance industry has exposed the big opportunity of growth and value extraction but at the 
same time imposed the various new challenges, which demand new skill set. \cite{Ref1} suggests that finance experts believe there 
is a huge potential in terms of value extraction from the financial big data.
They also believe that finance industry can benefit more than any other industry.  Historically, data was always there in some format either 
non-digital or digital. However, with digitization, this data has fallen into the prevalence of high volume of information, which we call as Big Data.
Dominant drivers for the actuality of big data in the finance industry are mainly customer call logs, social media, news feed, regulatory data etc. 
Call logs, news feed and etc. fall into the category of unstructured data which is identified as an area where we can extract vast amount of business value.


\indent

\cite{Ref2}  talks about the three V of big data in finance industry: volume, velocity and variety. 
\cite{Ref3} clearly depicts the amount of financial data pouring in the daily basis. 
TechNavioâ€™s forecast (Technavio 2016) predicts data will grow at a CAGR [compound annual growth rate] of 61 percent over the period of 2017-2021.
 According to the IDC financial insight 2016, every second there is around 10,000-payment card transaction and this number is expected to double by the end
 of this decade. The Capgemini/RBS Global payments study for 2012 suggests there was about 260 billion transactions in 2012 and is expected to grow between 15
 and 22 percent for developing countries. Main drivers contributing to the big data in the finance industry are Data growth, increasing scrutiny from regulators,
 digitization of financial products, changing the business model and increased customer insight platforms such as customer service. 
 \cite{Ref2} shows 76 percent of banks say the business driver for embracing big data is to enhance customer engagement, retention, and loyalty and seventy one percent
 of banks say that to increase their revenue, they need to better understand customers and big data will help them to do so. 

\indent

Thinking about the data strategy, the financial industry has taken the business-driven approach to a big data. According to the IBM report, all financial 
organizations are not keeping the same pace as peer industry is keeping. Today because of increased competition, customers always expect more personalized 
banking service and at the same time, there is increased regulatory surveillance which in result creates big pressure on finance industry to better utilize
 the value of Big data. To achieve better-personalized experience, many banks have started the initiative to utilize the information gained from the vast
 ocean of data to offer better-personalized products and gain competitive advantage.  Despite the fact that financial industry is data-driven,
 there is a gap in the amount of initiative financial industry has taken to extract the value out of big financial data.
 Technavio 2016 report has shown only 26 percent of financial organizations has focused on understanding the principal notation of Big data and most 
 of those 26 percent are still struggling to define the clear roadmap. This clearly concludes that finance industry lag behind their cross-industry peers 
 in using more varied data types. A good example to support this fact is that there are very less research and domain knowledge in extracting value out of 
 retail bank call logs.  

\indent

Big data technologies not only help in extracting the effective business value but analysis of unstructured data in conjunction with a wide variety of data set 
also helps in extracting commercial value. Big data in finance industry does not necessarily decode to valuable or actionable information.
The real benefit lies in developing the technologies, which can be used to extract business and commercial value.  
\cite{Ref4}  talks about what all advantage we can extract from the big data in the finance industry. Few examples are: Detection of false rumors that try to
 manipulate the finance market, Assessment of exposure to a reputational risk connected to consulting service offered by banks to their customer and Discover 
 topic trends, detect events, or support the portfolio optimization or asset allocation.  Big data based pattern recognition can also help in enhanced fraud 
 detection systems and prevention capability systems. Other benefits of utilizing big data include building a machine learning based algorithm to achieve higher 
 performance and accuracy in the trading algorithm and Enhanced market trading analysis. There has been proven research \cite{Ref5} which states more data increases
 accuracy and precision of simulations which is the backbone of financial modeling based analytic. This research \cite{Ref5} states modern modeling techniques
 are data hungry.In this project we will extract inference if more financial data can be used to have better prediction.  


\section{Use of structured financial data}


This reflects the data which has a higher degree of an organization such as a relational database where information/data is easily searchable and we can easily 
apply standard algorithm to extract patterns out of it. In this project we will be using Yahoo finance structured data. Examples of such data set include
yahoo financial data, trading applications, enterprise finance resource planner, Retail banking 
systems, Credit history database systems and other financial applications that use legacy application systems. Structured data always has a big advantage of 
being easily entered, stored, queried and analyzed. Most of the personal banking financial statements are stored in a structured way.
 Structured dataset combined with the distributed systems can be leveraged to achieve structured big data set on which we can run optimized SQL queries to
 retrieve patterns. \cite{Ref6}  discusses various SQL based ways to specify information quality in data which can be used to filter out the noise. In this project 
 we will be using structured data. 

\section{Various Challenges utilizing Big data value in finance industry }

There are multiple challenges and constraints in extracting value out of big financial data. The biggest challenge is old IT culture and infrastructure.
 The much financial organization still uses old IT infrastructure which is not compatible with the big data application thus fail to take advantage of big data.
 Other challenges include lack of skill set and data privacy and security. With the emergence of digitalization, customer data is saved persistently because of 
 which there has been continued concern regarding the customer privacy. Regulatory bodies guidelines on customer data are always ill-defined because of which is
 there is always a concern regarding the use of customer data.  In this project we will use standard python libraries to fetch financial data from yahoo finance. 
 Analysis will be done on the jupyter notebook. 
 
\section{Stock returns prediction - Literature Review}

Authors of \cite{Ref8} discuss the importance of stock price and returns prediction based on the data extraction of historic data. This research \cite{Ref8}
also shows historic financial data has definitive predictive relationship to the future value of stocks. Stock prediction always help investors to decide perfect
timing of buying or selling stocks. There are various data mining, artificial neural networks and machine learning techniques available for the stock price prediction based on the value
extraction from the historic financial data. Based on the complexity of stock price matrix, pricing mechanism is essentially a non linear complex system. 
Authors of \cite{Ref9} and \cite{Ref10} state many predictive algorithm is based on the fundamental analysis of macroeconomics and company fundamentals.
\cite{Ref11} states problem with the fundamental analysis is that it is too much focused on the intrinsic and lacks the quantitative aspect of the historic financial
data. On the broad category we can define stock prediction analysis is based on two types of analysis: qualitative and quantitative. Choice of analysis is mainly 
based on the fact if we want to have short term analysis or long term analysis. In this project we have have ten and sixteen years of training data and used close to 
one year of testing data. Since our analysis is based on the historic data we have chosen to do quantitative analysis. Quantitative analysis is based on the pattern 
extraction, fact that history repeats and future financial drivers can be extracted based on the historic data. Advantage of using quantitative analysis is that we 
can use statistical confidence interval to validate the analysis. 

\indent
There is a huge benefit of using machine learning algorithms in predicting stock prices. These algorithms made easy to cope up with the various financial events
such as mergers acquisitions, bankruptcy, fraud, political changes, market crashes, housing bubble, dot net bubble and etc. 
In this project we have used hybrid approach of combined CAPM [Capital asset pricing] model and machine learning algorithm to mine data of sixteen
and ten years of data and used close to 1 year of testing data. These machine learning algorithms can further be used to predict various financial events. 
Other approaches such as neural networks algorithms, SVM, logistic regression and multiple discriminant analysis can also be used to predict financial events. 
Example, \cite{Ref12} in their research they proved neural networks algorithms performs better in predicting financial events as compared to multiple discriminant 
analysis. There are other applications which use these algorithm to find predicted credit rating of a company. Credit rating plays a very important role 
doing qualitative analysis of the financial health of a company. On the other hand, accuracy of these algorithm is a big challenge because of the amount of huge data 
which it uses as input. Typically, accuracy of these algorithms is validated based on square root method. 

\indent
In this project we have used several years of data for analysis which involves more than hundred thousands of rows with multiple columns. Then this data is 
analyzed two dimensionally with the same set of market return rows. Since this analysis is calculation intense, In the end we also have performed root mean
square analysis. 

\indent
Over the past few years there has been drastic changes in the way stock market operates. With the emergence of advance web services, there has been powerful 
enhancement in the data communication between various financial application. Because of which there is ocean of real time data is available, thus machine learning 
algorithm, neural networks algorithms, SVM, logistic regression and multiple discriminant analysis needs to be smarter. Forecasting stocks and financial parameter 
is of great interest to the investors. As discussed earlier these algorithms needs to modified depending on the fact if we want to have short term profit or long 
term profit. 

\indent 
\cite{Ref13} has shown the very interesting analysis of comparing the prediction of stock market with the random walk hypothesis. Author of \cite{Ref13} ran an 
experiment in which he tossed a coin and recordes the results and mapped head with the company profit and tail with the company loss. Then result of this experiment 
was shown to the investors pretending these are the actual market profit and loss. Looking at the result graphs, investors believed it as a actual prediction. 
This research has shown the altogether different outlook which states stock price prediction and forecast can be fooled and stock prices are perfectly random in
nature. On this theory many researchers have classified profit based on three hypothesis:

\begin{itemize}

\item Weak form Efficient Market Hypothesis: The weak form of the hypothesis states one can not generate profit by just looking at patterns and trends of stock market.  
\item Semi Strong Efficient Market Hypothesis: The semi strong form of the hypothesis states only possible way of generating profit is via inside trading. 
\item Strong form Efficient Market Hypothesis: The strong strong form of the hypothesis states its not possible to generate profit since stock market behaves in perfect random way. 
\end{itemize}
 
 
However, if we are running root mean square analysis we can surely compare the accuracy of various algorithm and arrive at conclusion which algorithm 
is viable for prediction.


\section{Financial Data extraction}

In this section, we will discuss various technical requirements needed to achieve value extraction from the big data in the finance industry. 
There are various technical requirements such as data Acquisition, data quality, data extraction, data integration, decision support. 
In order to fulfill requirements, a hybrid approach combining computer science, algorithms, statistics, data mining, machine learning and pattern recognition 
study needs to be adopted. To explore the advantage of big data there have been initiatives like data virtualization, multi-document summarization, 
pattern recognition from LOGS and many start-ups have been emerged.  All big companies such as Microsoft, Google, IBM and Amazon are investing heavily in this 
field to leverage business and commercial value out of it. There has been changed in the industry pattern where financial industry is resorting big data to 
strategize their business. According to \cite{Ref3}  with a very rapid pace, the financial industry is utilizing big data advantage in investment analysis,
econometrics, risk assessment, fraud detection, trading, customer interaction analysis and behavior modeling. If we look at the Big promise the Big data holds 
in the finance industry, progress in this field is still in nascent stage and we expect more growth in upcoming years.  In this project we will discuss jupyter 
notebook based solution for Data extraction. 

\indent
In this project we have used jupyter notebook and rich python libraries to fetch financial stock data. Later in this paper we will discuss the stock data 
extration in detail. Later we will also discuss what are different ways to fetch stock data and will discuss few important functions which python libraries 
 
\section{Fetching financial stock data}

Fetching structured precise data is always a challenge. There are different ways to fetch the stock market data. In this project we will be fetching data from yahoo finance via python libraries which internally makes 
remote web service call to the yahoo web server. There are also other ways to fetch data such as:

\begin{itemize}
  \item Direct download of csv files from yahoo finance or google websites.
  \item Make web api call to download the data in the json/XML format
  \item Use python libraries to download data, which internally makes remote web service call to the yahoo web server. This is preferred way of doing since it allows you to save data to system variables directly. 
  \item Call yahoo or finance web service from the application. 
  \item Calling VBA function in excel to fetch yahoo stock data 
  \item Quandl best for using core financial data and this website also includes access to rich python libraries. 
  \item Google sheet has feature to fetch real time stock prices
  \item Install stocks macros in excel 
\end{itemize}

In this project we have exhaustively used python for data manipulation. Reasons for using python are:

\begin{itemize}
  \item Syntax is super easy which comes with very level of readability as compared to other programming languages.
  \item It is free and supports cross platform as python code can be called from any version of machine.
  \item Python has strong community support so if any problem is encountered, support is available online. 
  \item Python has powerful tools available such as statsmodels, matplotlib, Pandas,Numpy and SciPyfor calculation intense projects
\end{itemize}

Since we have exhaustively used the \texttt{get\_data\_yahoo} function from the \texttt{pandas\_datareader} python library we will briefly discuss the parameters it takes. 
Please note we utilized only those arguments which are relevant to the project requirements. From \cite{Ref7} parameter list as listed below:

\begin{itemize}

\item symbols : string, array-like object (list, tuple,Series), or DataFrame Single stock symbol (ticker), array-like object of symbols or DataFrame with index containing stock symbols.
\item start : string, (defaults to '1/1/2010' Starting date, timestamp. Parses many different kind of date representations (e.g., 'JAN-01-2010', '1/1/10', 'Jan, 1, 1980')
\item end : string, (defaults to today) Ending date, timestamp. Same format as starting  date.
\item \texttt{retry\_count} : int, default 3 Number of times to retry query request.
\item pause : int, default 0 Time, in seconds, to pause between consecutive queries of chunks. If single value given for  symbol, represents the pause between retries.
\item session : Session, default None requests.sessions.Session instance to be used
\item \texttt{adjust\_price}: bool, default False If True, adjusts all prices in hist data ('Open','High', 'Low','Close') based on 'Adj Close' price. Adds 'Adj Ratio' column and drops 'Adj Close'.
\item \texttt{ret\_index} : bool, default False If True, includes a simple return index 'Ret Index' in hist data.
\item chunksize : int, default 25 Number of symbols to download consecutively before intiating pause.
\item interval : string, default 'd' Time interval code, valid values are 'd' for daily, 'w' for weekly, 'm' for monthly and 'v' for dividend.
\end{itemize}

\indent 
In our analysis, for the symbol parameter we are passing ticker symbol one at a time. Though, we have an option to pass multiple tickers as an array argument. 
We are using \texttt{get\_data\_yahoo} function and utilizing only
first three paramters: symbols, start and end.  This function returns YahooDailyReader object which can further be manipulated to get Open, High, Low, Close, Adj Close
and Volume stock values. Since default number of retry count is three we will be using this default value. Default value of pause which is zero is also good with
respect to our requirement so we will not pass this as argument. Session argument should be used when we are handling multiple request in parallel  in the code
since our project we just need one session so we will not use this argument. \texttt{adjust\_price} is not required in our analysis since we are interested only in returns
which can be fetched using \texttt{pct\_change()} function. Since return index is of no use in calculating the returns, we will not use this argument.
Argument chunk size is used to modify number of consecutive downloads of stocks since we are just using single ticker so this argument is of no use.
This function uses interval also as a parameter since we are only interested in daily values and daily value is the default 
interval so we didn't pass this argument in the function call.We could also use the contemporary google function which is \texttt{get\_data\_google}.
Arguments which goes to the \texttt{get\_data\_google} are symbols, start, end, \texttt{retry\_count}, pause, chunksize and session since we are not using 
\texttt{get\_data\_google} function in our project we will not discuss these in detail. 

\section{Introduction to CAPM model}

CAPM [Capital asset pricing model] model was developed by William Sharpe and John Lintner in 1964. This model is considered so powerful that it is being used in current 
prediction models. There are few advantages of using CAPM model as compared to other pricing models:

\begin{itemize}
\item This model is a single dimensional model and easy to use, still powerful to model capital asset pricing. 
\item Since this model is based on the market portfolio and risk free rate, this model removes unsystematic risk.  
\item We can run root mean square algorithm to validate the algorithm.
\item This model provides a flexibility to utilize various risk free rates and run model for various time range. 
\item This model can be applied to various financial objects such as stocks, put option, call option, bonds, and etc

\end{itemize}

This model can be used to evaluate the theoretical expected return on a security, security can be any financial object such as stocks, put option, call option, bonds, and etc. 
In CAPM model we evaluate how much financial object is sensitive to the market using statistical analysis. Then this sensitivity which is also known as beta is used
to find the expected return on security. This expected return can be on daily basis, weekly basis, monthly basis or yearly. Here is the formula to evaluate expected return:


$$E(R_{i}) = r_{f} + \beta_{i}(E(r_{m}) - r_{f})$$

\indent
Where
\indent 

 
\begin{itemize} 
\item $E(R_{i})$ is expected return 
\item $r_{f}$ is risk free interest rate example: Government bond
\item $E(r_{m})$ is return on market example SP 500
\item $\beta_{i}$ is sensitivity of stock with respect to market  
\end{itemize}


$\beta_{i}$ can further be defined how much stock is sensitive to the stock market. Example if $\beta_{i}$  for a particular stock is two it means if market goes up by five percent then stock will go up by ten percent and if market goes down by two percent then stock will go down by ten percent. In terms of statistics $\beta_{i}$  is defined as:

$$\beta_{i}  = \frac{Cov(R_{i},r_{m})}{Var(r_{m})}$$

\indent
Where covariance and variance are defined as
\indent

\begin{itemize} 
\item $cov_{x,y}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(y_{i}-\bar{y})}{N-1}$

\item $var^2 = \frac{\displaystyle\sum_{i=1}^{n}(x_i - \mu)^2} {n}$
\end{itemize}

\indent 
$\beta_{i}$ matrix can be used to illustrate $\beta_{i}$ in a following way:

$$\bordermatrix{\text{}&\beta_{i} &Market Return  &Expected Return\cr
                Row 1& +2   &  +5 \%  &  +10 \%\cr
                Row 2& -2   &  +5 \%  &  -10 \%\cr
                Row 3& +0.5 &  +4 \%  &  +2  \%\cr
                Row 4& +0.5 &  -4 \%  &  -2  \%}$$

\indent

Above matrix suggests how expected returns can be correlated with the the $\beta_{i}$. Example if for certain company has $\beta_{i}$ of +2 and 
market returns is +5 \% then company\textquotesingle s expected returns can be predicted as +10 \%. Please note $\beta_{i}$ can be positive as well as negative. 

                
                
\section{Proposed Analysis}
 
\indent

In this project we will utilize structured data and use CAPM [Capital asset pricing model] to statistically find the expected daily return of selected technological stocks: Amazon and Yahoo.
This daily expected return can be used to predict next day stock value given the condition we have current stock price. 
Following formula can be used to predict next day stock price:


\begin{algorithm}
\textbf{\textit{Next Day stock price =}}: Today stock price * (1+Daily expected return)\newline
\end{algorithm}

\indent
Daily expected return will be calculated using CAPM model. Daily expected return sensitivity in CAPM terminology is also known as beta. In this project beta will be calculated 
based on two time frames:

\begin{algorithm}
\textbf{\textit{Time frame 1}}: [01/01/2000 to 12/31/2016] 16 years of data\newline
\textbf{\textit{Time frame 2}}: [01/01/2006 to 12/31/2016] 10 years of data\newline
\end{algorithm}

\indent
Thus we we will have 2 $\beta_{i}$:

$$\beta_{1}  = \frac{Cov(R_{1},r_{m1})}{Var(r_{m1})}$$

\indent 
$$\beta_{2}  = \frac{Cov(R_{2},r_{m2})}{Var(r_{m2})}$$

\indent
Where
\indent

\begin{itemize} 
\item $\beta_{1}$ is $\beta$ based on time frame 1
\item $\beta_{2}$ is $\beta$ based on time frame 2
\item $R_{1}$ is actual return  based on time frame 1
\item $r_{m1}$ is a mean market return based on time frame 1
\item $R_{2}$ is actual return  based on time frame 2
\item $r_{m2}$ is a mean market return based on time frame 2
\end{itemize}

\indent
Above two time frames will be our training data set.We will run two analysis: one on training time frame 1 and other on training time frame 2 to arrive at predicted
CAPM variables. Then we will use this training data set to predict stock returns for test data set which will comprise of time frame:

\begin{algorithm}
\textbf{\textit{Test data time frame}}: 01/01/2017 to 11/16/2017\newline
\end{algorithm}

Then we will run the statistically analysis on the test data to evaluate if 16 years of training data produced more accurate result or else it added noise compared
to 10 years of training data. Please note this is purely a quantitative analysis not qualitative. Actual returns can also be impacted by a qualitative factors
such as mergers acquisitions, bankruptcy, fraud, political changes, market crashes, housing bubble, dot net bubble and etc.  

\section{Proposed Algorithm}

Code is written purely in python language and used the powerful rich python libraries such as statsmodels, matplotlib, Pandas,Numpy and SciPyfor. 
We have used jupyter notebook as interpreter tool to python. Code is started by importing above mentioned rich python libraries. Since we are interested 
only in technological stocks: Amazon and yahoo we need to initialize they stock ticker with the python variable. In CAPM model we need to know the market
return in order to know the stock sensitivity we will also initialize market ticker with SP 500 index. As discussed above we will be using \texttt{get\_data\_yahoo} 
function from the \texttt{pandas\_datareader} and in this project we will be only utilizing only first three parameters which is stock ticker, start date and 
end date. For first iteration we will be using \texttt{get\_data\_yahoo}  to fetch stocks and market returns for time frame 1. For having better understanding 
of how the data looks when fetched using \texttt{get\_data\_yahoo}  function, we will have amazon financial data matrix calculated like:

$$amazonData = dr.\texttt{get\_data\_yahoo}(amazon,\texttt{start\_date},\texttt{end\_date})$$

\indent
Where 
\indent

\begin{itemize} 
\item dr is \texttt{pandas\_datareader}.data class
\item amazon is stock ticker for amazon which is \textquotesingle AMZN \textquotesingle
\item \texttt{start\_date} is start date of time frame 1: 01/01/2000
\item \texttt{end\_date} is end date of time frame 1: 12/31/2016
\end{itemize}

and synopsis of above amazon data looks like:

\indent

$$\bordermatrix{\text{}      &Open      &High      &Low      &Close     &Adj      &Volume   \cr
                23-Dec-16    & 764.54    &766.50   &757.98   &760.59    &760.59   &1976900  \cr
                27-Dec-16    & 763.40 	 &774.65   &761.20 	 &771.40 	&771.40   &2638700  \cr
                28-Dec-16    & 776.25 	 &780.00   &770.50 	 &772.13 	&772.13   &3301000  \cr
                29-Dec-16    & 772.40 	 &773.40   &760.84 	 &765.15 	&765.15   &3153500  \cr
                30-Dec-16    & 766.46 	 &767.40   &748.28 	 &749.86    &749.86   &4139400 \cr}$$
                
\indent 


Similarly using \texttt{get\_data\_yahoo} we will fetch Yahoo and market returns. Since we are interested in daily return, we fetched the daily data from yahoo 
finance which is evident from the above result data set. Now lets find the percentage change on the daily Close value to get the percentage change array which 
in finance terminology will be daily return on stock. For finding the percentage change we are using \texttt{pct\_change()} function on the close column of result set. 
This function can be elaborated as follows:

\indent

$$\texttt{return\_amazon} = amazonData.Close.pct_change()[1:]$$
$$\texttt{return\_yahoo}  = yahooData.Close.pct_change()[1:]$$
$$\texttt{return\_market} = marketData.Close.pct_change()[1:]$$

\indent 

\texttt{return\_amazon}, \texttt{return\_yahoo} and \texttt{return\_market} are two dimensional arrays and we need to convert them to single dimensional array 
in order to run statistical analysis. We can use dot values method to extract single dimensional array out of 2 dimensional array. This operation can be elaborated
as follows:

\indent

$$\texttt{X\_amazon\_actualReturns} = \texttt{return\_amazon\_testing}.values$$
$$\texttt{X2\_yahoo\_actualReturns} = \texttt{return\_yahoo\_testing}.values$$
$$\texttt{Y\_market\_actualReturns} = \texttt{return\_market\_testing}.values$$

\indent

Please note these are actual returns - fetched from yahoo finance. Now in order to evaluate expected return for the testing period based on the calculated beta we need 
to calculate the risk free rate $r_{f}$ as mentioned above in the CAPM formula. Please note \texttt{get\_data\_yahoo} formula will fetch the annualized rate but here we are dealing 
with the daily returns so this needs to be normalized to daily rate. Here we are using Treas Yld Index-10 Yr Nts bond. Ticker symbol for Treas Yld Index-10 Yr Nts bond
is \texttt{\^TNX}. Please note \texttt{get\_data\_yahoo} will return columns: Open, High, Low, Close, Adj Close and Volume. Dot values will convert to 2 dimensional array 
and then used index [0][4] to fetch annual rate. Detailed code with comments is mentioned on jupyter notebook. 

Converstion of annualized return to daily return can be done using following formula:

\indent
$$riskFreeDailyRate = (1+riskFreeAnnualRate) \hat{} (1/365) -1 $$
\indent 

Now we need to copy the content of \texttt{X\_amazon\_actualReturns} to new array \texttt{X\_amazon\_predictedReturns} and initialized each element in \texttt{X\_amazon\_predictedReturns} using 
CAPM model as discussed above in Introduction:
\indent
$$\texttt{X\_amazon\_predictedReturns} = list(\texttt{X\_amazon\_actualReturns})$$

\indent 
We will do the same for Yahoo stocks:
\indent 
$$\texttt{X2\_yahoo\_preddictedReturns} = list(\texttt{X2\_yahoo\_actualReturns})$$
\indent 


In the code we have run the while loop and each element of \texttt{X\_amazon\_predictedReturns} and \texttt{X2\_yahoo\_preddictedReturns} is assigned the value
based on CAPM model. Now we have two returns arrays for amazon stocks based on sixteen years of data: 

\begin{itemize} 
\item \texttt{X\_amazon\_actualReturns} are the actual returns  
\item \texttt{X\_amazon\_predictedReturns} are returns based on the CAPM model. 
\end{itemize}

Similarly we have two returns arrays for yahoo stocks based on sixteen years of data: 

\begin{itemize} 
\item \texttt{X2\_yahoo\_actualReturns} are the actual returns 
\item  \texttt{X2\_yahoo\_preddictedReturns} are the returns based on the CAPM model. 
\end{itemize}

Now we can utilize \texttt{mean\_squared\_error} function from the sklearn.metrics python library to find how predicted returns are deviated from the 
actual returns. We will run this function on both stocks, amazon and yahoo:

$$ a1 = \texttt{Y\_market\_actualReturns} $$
$$ a2 = \texttt{X\_amazon\_predictedReturns} $$
$$ y1 = \texttt{Y\_market\_actualReturns} $$
$$ y2 = \texttt{X2\_yahoo\_preddictedReturns} $$

$$ \texttt{rms\_amazon} = sqrt(\texttt{mean\_squared\_error}(a1,a2)) $$
$$ \texttt{rms\_yahoo}  = sqrt(\texttt{mean\_squared\_error}(y1,y2 ))$$

Here is the root mean square values for both the stocks under sixteen years of data case:

\begin{itemize} 
\item Root mean square error for Amazon stocks analysis based on 16 years of data 0.0013770 or 0.137 percent 
\item Root mean square error for Yahoo stocks analysis based on 16 years of data  0.0014313 or 0.143 percent 
\end{itemize} 

Now we run the same analysis as discussed above for the ten years of data and will validate how much predicted stocks returns 
based on the ten years of data are deviated from the actual returns using root mean square method. 
Please note testing data set remains the same we are just using different training data set. 
This will let us compare if sixteen years of data is of more worth in predicting stock returns or it added noise to the analysis:

\begin{itemize} 
\item Root mean square error for Amazon stocks analysis based on 10 years of data 0.0005310 or 0.053 percent 
\item Root mean square error for Yahoo stocks analysis based on 10 years of data  0.0014910 or 0.149 percent 
\end{itemize}  

Above analysis is purely quantitative and does not include any elements of qualitative analysis. It shows predicting yahoo stock price or 
its returns based on the sixteen years of data or ten years of data - both resulted in almost same results. However things are totally 
different for the amazon stocks, recent ten years of amazon stocks data produced more accurate results as compared to using recent sixteen years 
of data. Author of \cite{Ref14} agrees with the fact that most recent financial data are the better predictors of the future price returns. Though, 
in the \cite{Ref14} author has used the neural networks and support vector machine for prediction. Author also stressed that neural networks algorithm
produced better accuracy than other machine learning algorithms. 

\section{Three paradigms of prediction}

Data prediction and analysis done in this project is purely quantitative. However there are other paradigms of predictions also which we will discuss here. 
Example in above analysis we totally missed the qualitative aspect of the data. This is why it explains recent data on amazon stocks produced better results. 
Here are the other prediction paradigms explained by the author of \cite{Ref15} : 

\begin{itemize}
\item Quantitative research based prediction. This is a method where we utilize statistical tools to arrive at predictive value based on data
\item Quantitative research based prediction. This is a method where we utilize conceptual knowledge to arrive at predicted value. Example of such study would be
      prediction of stocks based on the events such as mergers acquisitions, bankruptcy, Fraud, political changes, market crashes, housing bubble, dot net bubble and etc.
\item Mixed research based prediction. This is a hybrid method where we utilize both qualitative and quantitative results to predict result. 
\end{itemize}

In this project we used quantitative based approach to validate the fact if more data is good for prediction or it adds noise. Result of this project also showed
there is a importance of recent data in predicting results. This is also validated by the research done under \cite{Ref14}


\section{Conclusion}
In this project, analysis is based on two technological stocks: yahoo and amazon. We can extend our study to more diverse portfolio by including more stocks 
from various industries. Technological stocks tend to be more volatile than other stocks. Since this project is purely quantitative based prediction we 
deliberately chosen the technological stocks to leverage their volatility. More accurate prediction could also be made by encapsulating qualitative based prediction
 in the analysis which is more like a hybrid approach. Such hybrid approaches includes assigning weight to each predictions and taking cumulative result.  
 As the part of future work we can also compare results across industry and arrive at conclusion which industry is more stable in prediction. Comparison can based on 
the root mean square analysis which is discussed in this project report.  


Main objective of doing this project is to know the importance of big data in predicting financial variables. Analysis of this project is based on two stocks:
amazon and yahoo. We started this project report with the discussion of importance of big data in financial industry. In the introduction we discussed how various
industries are investing in the Big Data to attain higher standards in terms of quality and customer satisfaction. Then we discussed what are the various types 
of data available: structured and unstructured. Since in this project we utilized only structured data so it was discussed deeply. This project report also touch base
with various challenges financial industry takes in utilizing the value of big data. As the part of literature review we reviewed various researches done in the field of 
stock returns prediction. In the financial data extraction section we reviewed various technical requirements need for financial data extraction. As the part 
of data analysis for this project we discussed what are the various ways to fetch live stock data from yahoo or google server. In this project  we used the 
rich financial python libraries for the analysis so we discussed them in details in this report.  Financial model which we chose for the prediction is the CAPM model which 
is explained theoretically in this report.  There are two different section where we discussed the proposed analysis and proposed algorithm. We finally concluded the 
report by discussing three paradigms of prediction. In the end we also mentioned what further can be done under future work section. 
 

\begin{acks}

  The author would like to thank Dr. Gregor von Laszewski and TAs for their
  support and suggestions to write this paper. TAs and professor are very good in terms of providing valuable 
  guidance and suggestion in a very prompt fashion. 

\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 


\newpage
\appendix
\section{HID 301:Gagan Arora}
\begin{itemize}
  \item Identified Project topic.
  \item Collected the python financial libraries.
  \item fetched data from yahoo finance
  \item Studied, designed and reviewed CAPM model
  \item Implemented CAPM model using python libraries 
  \item Created  project report  
\end{itemize}

\section{Code Reference}
All code, notebooks and files for this project can be found in the githup repository:
\url{https://github.com/bigdata-i523/hid301/blob/master/project/finalProject.ipynb}



